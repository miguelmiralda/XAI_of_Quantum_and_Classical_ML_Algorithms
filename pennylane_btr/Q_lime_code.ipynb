{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Classical LIME\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 0: DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Removes HTML tags and converts to lowercase.\n",
    "    \"\"\"\n",
    "    # Remove anything between <...> tags, then lowercase the text\n",
    "    cleaned = re.sub(r'<.*?>', '', text).lower()\n",
    "    return cleaned\n",
    "\n",
    "def load_imdb_subset(\n",
    "    num_samples=5000, \n",
    "    min_df=1, \n",
    "    max_features=15, \n",
    "    stopwords_option=True,\n",
    "    stop_words = 'english'\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads a subset of IMDb data, returns:\n",
    "      - X_train, X_test (lists of text)\n",
    "      - y_train, y_test (0/1 sentiment)\n",
    "      - vectorizer (CountVectorizer)\n",
    "    \n",
    "    Now with text cleaning for HTML, lowercase, etc.\n",
    "    \"\"\"\n",
    "    data = load_files(\n",
    "        \"C:/Users/migue/Downloads/aclImdb_v1/aclImdb/train\",\n",
    "        categories=['pos','neg'], \n",
    "        encoding=\"utf-8\", \n",
    "        decode_error=\"replace\"                  \n",
    "    )\n",
    "    X_text_all, y_all = data.data, data.target\n",
    "\n",
    "    # Clean text (HTML removal + lowercase)\n",
    "    X_text_all = [clean_text(txt) for txt in X_text_all]\n",
    "\n",
    "    # Shuffle & truncate to num_samples\n",
    "    full_idx = np.arange(len(X_text_all))\n",
    "    #np.random.shuffle(full_idx)\n",
    "    subset_idx = full_idx[:num_samples]\n",
    "    X_text = [X_text_all[i] for i in subset_idx]\n",
    "    y = y_all[subset_idx]\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Vectorizer: presence/absence\n",
    "    if stopwords_option:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=stop_words, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            binary=True, stop_words=None, \n",
    "            min_df=min_df, max_features=max_features\n",
    "        )\n",
    "\n",
    "    vectorizer.fit(X_train)\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "\n",
    "#def train_XGBoost_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost classifier on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    # Use log(len(y_train)) as n_estimators (rounded to an int)\n",
    "    clXGB = XGBClassifier(\n",
    "        #booster=\"gblinear\",\n",
    "        objective=\"binary:logistic\", \n",
    "        eval_metric=\"logloss\", \n",
    "        random_state=42, \n",
    "        n_estimators=int(round(math.log(len(y_train)))),\n",
    "        learning_rate=0.1, \n",
    "        max_depth=3\n",
    "    )\n",
    "    clXGB.fit(X_train_bow, y_train)\n",
    "    return clXGB\n",
    "\n",
    "#def get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_xgboost_ns{num_samples}_mf{max_features}_sw{stopwords_option}_xgboost_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached xgboost from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clXGB = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clXGB = train_XGBoost_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clXGB, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clXGB\n",
    "\n",
    "def train_logistic_classifier(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train_bow, y_train)\n",
    "    return clf\n",
    "\n",
    "def get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stop_words):\n",
    "    \"\"\"\n",
    "    Checks if a classifier trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_classifier_ns{num_samples}_mf{max_features}_sw{stop_words}_logistic_classifier_seed42.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached logistic from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached classifier found. Training a new one...\")\n",
    "        clf = train_logistic_classifier(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(clf, f)\n",
    "        print(\"Cached classifier saved as\", filename)\n",
    "    return clf\n",
    "\n",
    "#def train_lasso_regression(X_train, y_train, vectorizer):\n",
    "    \"\"\"\n",
    "    Trains a logistic regression on the binary presence/absence of words.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    X_train_bow = vectorizer.transform(X_train)\n",
    "    lasso_model = Lasso(alpha=0.5)\n",
    "    lasso_model.fit(X_train_bow, y_train)\n",
    "    return lasso_model\n",
    "    \n",
    "\n",
    "#def get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha):\n",
    "    \"\"\"\n",
    "    Checks if a Lasso model trained with the given parameters exists.\n",
    "    If so, load it; otherwise, train it and save it.\n",
    "    \"\"\"\n",
    "    filename = f\"cached_lasso_ns{num_samples}_mf{max_features}_sw{stopwords_option}_seed42_alpha{alpha}.pkl\"\n",
    "    if os.path.exists(filename):\n",
    "        print(\"Loading cached Lasso model from\", filename)\n",
    "        with open(filename, 'rb') as f:\n",
    "            lasso_model = pickle.load(f)\n",
    "    else:\n",
    "        print(\"No cached Lasso model found. Training a new one...\")\n",
    "        lasso_model = train_lasso_regression(X_train, y_train, vectorizer)\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(lasso_model, f)\n",
    "        print(\"Cached Lasso model saved as\", filename)\n",
    "    return lasso_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSICAL LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#CHANGE clXGB TO clf IF WE WANT LOGISTIC INSTEAD OF XGBOOST\n",
    "def run_classical_lime(\n",
    "    text_sample, clXGB, vectorizer, \n",
    "    k_features=10, num_samples=500\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs classical LIME on a single text instance.\n",
    "    Returns the top (word, weight) pairs.\n",
    "    \"\"\"\n",
    "    class_names = [\"negative\", \"positive\"]\n",
    "    explainer = LimeTextExplainer(class_names=class_names, feature_selection=\"auto\")\n",
    "\n",
    "    def predict_proba(texts):\n",
    "        bow = vectorizer.transform(texts) \n",
    "        return clXGB.predict_proba(bow)\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        text_sample,\n",
    "        predict_proba,\n",
    "        num_features=k_features,\n",
    "        num_samples=num_samples  # e.g. 300 or 500\n",
    "    )\n",
    "    return explanation.as_list()  # list of (word, weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-LIME Pi (Flip Only 1->0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_classifier(features, weights, bias=0.0, threshold=0.01):\n",
    "    # Ensure inputs are 1D arrays\n",
    "    features = np.array(features).flatten()\n",
    "    weights = np.array(weights).flatten()\n",
    "    \n",
    "    # Zero out small weights\n",
    "    sparse_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
    "    score = bias + np.dot(features, sparse_weights)\n",
    "    return float(1 / (1 + np.exp(-score)))\n",
    "\n",
    "def encode_and_flip(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Encode features -> quantum circuit.\n",
    "    FLIP ONLY if bit == 1 at flip_index (1->0).\n",
    "    \"\"\"\n",
    "    num_qubits = len(features)\n",
    "    dev = qml.device(\"default.qubit\", wires=num_qubits, shots=shots)\n",
    "\n",
    "    @qml.qnode(dev)\n",
    "    def circuit():\n",
    "        for i, f in enumerate(features):\n",
    "            if i == flip_index: # and f == 1:   ;;  This line is the original code commented out\n",
    "                # 1->0 => RY(0),\n",
    "                #theta = 0\n",
    "                #My suggestion: \n",
    "                theta = f * (np.pi / 2)            \n",
    "                qml.PauliX(wires=i)\n",
    "            else:\n",
    "                theta = f * (np.pi / 2)\n",
    " \n",
    "            qml.RY(theta, wires=i)\n",
    "            \n",
    "        return qml.probs(wires=range(num_qubits))\n",
    "\n",
    "    return circuit()\n",
    "\n",
    "def sample_state(probabilities):\n",
    "    \"\"\"\n",
    "    Sample an integer state index from the distribution.\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    cumsum = 0.0\n",
    "    for idx, p in enumerate(probabilities):\n",
    "        cumsum += p\n",
    "        if r <= cumsum:\n",
    "            return idx\n",
    "    return len(probabilities) - 1\n",
    "\n",
    "def measure_and_map_to_classical(features, flip_index=None, shots=None):\n",
    "    \"\"\"\n",
    "    Run the circuit, measure, return a binary array for the top-likelihood state.\n",
    "    \"\"\"\n",
    "    probs = encode_and_flip(features, flip_index=flip_index, shots=shots)\n",
    "    measured_state = sample_state(probs)\n",
    "    num_qubits = len(features)\n",
    "    bin_string = f\"{measured_state:0{num_qubits}b}\"\n",
    "    return [int(bit) for bit in bin_string]\n",
    "\n",
    "\n",
    "def quantum_lime_explanation(\n",
    "    features, weights, bias=0.0, shots=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Flip only features that are 1 -> 0.\n",
    "    Return array of shape (n_features,) with:\n",
    "       Delta f_k = (original_pred - new_pred).\n",
    "    \"\"\"\n",
    "    original_pred = classical_classifier(features, weights, bias=bias)\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        new_vec = measure_and_map_to_classical(features, flip_index=i, shots=None)\n",
    "        new_pred = classical_classifier(new_vec, weights, bias=bias)\n",
    "        return original_pred - new_pred\n",
    "\n",
    "    # Flip only bits that are 1\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) #if val == 1 # ;  This is the original code commented out \n",
    "        }\n",
    "        for future in futures:\n",
    "            i = futures[future]\n",
    "            contributions[i] = future.result()\n",
    "\n",
    "    return contributions\n",
    "\n",
    "#def quantum_lime_explanation(features, clf, lasso, shots=None):\n",
    "    \n",
    "    # Reshape features for prediction (ensure 2D array)\n",
    "    features_reshaped = np.array(features).reshape(1, -1)\n",
    "    original_pred = clf.predict_proba(features_reshaped)[0, 1]\n",
    "    contributions = np.zeros(len(features))\n",
    "\n",
    "    def flip_and_predict(i):\n",
    "        # Create a new feature vector with feature i flipped from 1 to 0\n",
    "        new_vec = features.copy()\n",
    "        new_vec[i] = 0\n",
    "        new_vec_reshaped = np.array(new_vec).reshape(1, -1)\n",
    "        new_pred = lasso.predict(new_vec_reshaped)[0]\n",
    "        return original_pred - new_pred\n",
    " \n",
    "    # Flip only bits that are 1\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = {\n",
    "            executor.submit(flip_and_predict, i): i\n",
    "            for i, val in enumerate(features) if val == 1\n",
    "        }\n",
    "        for future in futures:\n",
    "            i = futures[future]\n",
    "            contributions[i] = future.result()\n",
    "\n",
    "    return contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTAL ROUTINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment( #Did I change these numbers? check if i fcked up smth here!!!!!!!!!!\n",
    "    num_samples=10,\n",
    "    min_df=1,\n",
    "    max_features=15,\n",
    "    stopwords_option=True,\n",
    "    lime_num_samples=30,\n",
    "    shots=None,\n",
    "    n_test_explanations=10,\n",
    "    stop_words = None\n",
    "):\n",
    "    \"\"\"\n",
    "    1) Load data with given params (includes text cleaning)\n",
    "    2) Train logistic classifier\n",
    "    3) Evaluate test accuracy\n",
    "    4) Pick n_test_explanations random samples\n",
    "    5) For each, run classical LIME vs. Q-LIME Pi\n",
    "    6) Return summary stats\n",
    "    \"\"\"\n",
    "    # A) Load data\n",
    "    X_train, X_test, y_train, y_test, vectorizer = load_imdb_subset(\n",
    "        num_samples=num_samples,\n",
    "        min_df=min_df,\n",
    "        max_features=max_features,\n",
    "        stopwords_option=stopwords_option,\n",
    "        stop_words = stop_words\n",
    "    )\n",
    "    # B) Train model\n",
    "    clf  = get_cached_logistic(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "\n",
    "    #clXGB = get_cached_xgboost(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option)\n",
    "\n",
    "    # Evaluate\n",
    "    X_test_bow = vectorizer.transform(X_test)\n",
    "    #test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test_bow))\n",
    "\n",
    "    logistic_weights = clf.coef_[0]\n",
    "    bias = clf.intercept_[0]\n",
    "\n",
    "\n",
    "    #IT ONLY GIVES 1 WEIGHT NOT 15\n",
    "    #logistic_weights = clXGB.coef_[0]\n",
    "    #bias = clXGB.intercept_[0]\n",
    "\n",
    "\n",
    "    #lasso_model = get_cached_lasso(X_train, y_train, vectorizer, num_samples, max_features, stopwords_option, alpha=0.1)\n",
    "    \n",
    "\n",
    "    # We'll track times & top-feature overlap\n",
    "    lime_times = []\n",
    "    qlime_times = []\n",
    "    overlaps = []\n",
    "    instance_local_accuracies = []\n",
    "\n",
    "    # Random samples for explanation\n",
    "    #n_test = len(X_test)\n",
    "    sample_indices = [5,6,12,11,10, 0, 1, 2, 3, 4]\n",
    "    #random.sample(range(n_test), n_test_explanations)\n",
    "\n",
    "    for idx in sample_indices:\n",
    "        text_sample = X_test[idx]\n",
    "        y_true = y_test[idx]\n",
    "\n",
    "        # 1) Classical LIME\n",
    "        start_lime = time.time()\n",
    "        explanation_lime = run_classical_lime(\n",
    "            text_sample, clf, vectorizer, \n",
    "            k_features=15, num_samples=lime_num_samples\n",
    "        )\n",
    "\n",
    "        bow = vectorizer.transform([text_sample])\n",
    "        bin_features = bow.toarray()[0]\n",
    "\n",
    "        y_pred = clf.predict(bow)[0]\n",
    "        instance_accuracy = int(y_pred == y_true)\n",
    "        instance_local_accuracies.append(instance_accuracy)\n",
    "\n",
    "        #explanation_lime = run_classical_lime(\n",
    "        #    text_sample, clf, vectorizer, \n",
    "        #    k_features=15, num_samples=lime_num_samples\n",
    "        #)\n",
    "        lime_time = time.time() - start_lime\n",
    "        lime_times.append(lime_time)\n",
    "\n",
    "        # parse top features\n",
    "        lime_dict = dict(explanation_lime)\n",
    "        top_words_lime = sorted(\n",
    "            lime_dict.keys(),\n",
    "            key=lambda w: abs(lime_dict[w]),\n",
    "            reverse=True\n",
    "        )[:5]\n",
    "\n",
    "        # 2) Q-LIME Pi\n",
    "        \n",
    "\n",
    "        start_qlime = time.time()\n",
    "        #contributions_qlime = quantum_lime_explanation(bin_features, clf, lasso_model, shots=shots)\n",
    "        contributions_qlime = quantum_lime_explanation(\n",
    "            bin_features, logistic_weights, bias=bias, shots=shots)\n",
    "\n",
    "\n",
    "        contributions_lime_abs = [(word, abs(score)) for word, score in explanation_lime] # Absolute values for comparison; This is a tuple. PROB SHOULD MAKE QLIME A TUPLE TOO!\n",
    "        \n",
    "        unsorted_contributions_qlime_abs = tuple(\n",
    "            (word, abs(score)) for word, score in zip(vectorizer.get_feature_names_out(), contributions_qlime)) # Absolute values for comparison\n",
    "        \n",
    "        contributions_qlime_sorted = tuple(\n",
    "        sorted(unsorted_contributions_qlime_abs, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "        #print(\"X_test_bow\",X_test_bow)\n",
    "          \n",
    "        print(\"text sample\", text_sample, \"bin_features\", bin_features)\n",
    "        #, \"vectorizer\", vectorizer.get_feature_names_out(), \"contributions_qlime_abs\", contributions_qlime_abs, \"Contributions_Lime\", top_words_lime\n",
    "     \n",
    "        print(\"Classical LIME Explanation:\")\n",
    "        for word, weight in contributions_lime_abs:\n",
    "            print(f\"Word: {word}, Importance: {weight}\")\n",
    "\n",
    "        print(\"\\nQ-LIME Pi Explanation:\")\n",
    "        for word, weight in contributions_qlime_sorted:\n",
    "            print(f\"Word: {word}, Importance: {weight}\")\n",
    "        \n",
    "        #print(\"\\n weights\", clf.coef_[0])\n",
    "        qlime_time = time.time() - start_qlime\n",
    "        qlime_times.append(qlime_time)\n",
    "\n",
    "        # top 5 (by absolute value)\n",
    "        nonzero_indices = [\n",
    "            (i, abs(contributions_qlime[i])) \n",
    "            for i in range(len(contributions_qlime))\n",
    "        ]\n",
    "        top_indices_qlime = sorted(nonzero_indices, key=lambda x: x[1], reverse=True)[:5]\n",
    "        top_words_qlime = [\n",
    "            vectorizer.get_feature_names_out()[i2]\n",
    "            for (i2, val) in top_indices_qlime\n",
    "        ]\n",
    "\n",
    "        # measure overlap\n",
    "        overlap = set(top_words_lime).intersection(set(top_words_qlime))\n",
    "        overlaps.append(len(overlap))\n",
    "\n",
    "    # Summary\n",
    "    results = {\n",
    "        \"local_accuracy\": np.mean(instance_local_accuracies),\n",
    "        \"lime_time_avg\": round(np.mean(lime_times), 4),\n",
    "        \"qlime_time_avg\": round(np.mean(qlime_times), 4),\n",
    "        \"overlap_avg\": round(np.mean(overlaps), 4),\n",
    "    }\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "Running experiment with: num_samples=500, max_features=20, stopwords=True, lime_num_samples=300, shots=100,stop_words=english,n_test_explanations=5\n",
      "No cached classifier found. Training a new one...\n",
      "Cached classifier saved as cached_classifier_ns500_mf20_swTrue_logistic_classifier_seed42.pkl\n",
      "text sample having been driven out of the house and into the theater by the sweltering heat, i could not have been more pleased. the road to perdition, directed by sam mendes (american beauty), is destined to become one of the greatest movies of all time. perhaps i'm just getting old; perhaps i've just seen the same themes recycled time and again. but this movie is indeed different.the story opens with young michael sullivan jr. facing out to the sea, contemplating the duality of his father's legacy -- one of the best men to ever live, one of the most evil. this duality snakes its way throughout the movie. the story revolves around crime boss john rooney (paul newman) and michael sullivan (tom hanks), the young man rooney once took in and who now serves as his personal \"angel of death.\" rooney is tied by blood to his own son, but tied by love and loyalty to michael. young michael jr., intrigued by the stories he reads, steals away in his father's car one night while dad goes off to \"work\" with connor rooney, heir to the family \"business.\" connor lets the situation get out of hand, and what was meant only to be a warning turns into murder -- witnessed by michael jr. upon the discovery that young michael has seen what he should not have seen, the plot is set in motion as conflicting loyalties collide. soon, michael sr. is on the run with his young son, pursued by contract killer harlen \"the reporter\" maguire (jude law).i will disclose no further details in order to avoid any potential spoilers. however, i strongly encourage viewers to examine the many dualities that present themselves in the movie: problems between sons and fathers (michael sr & jr., john rooney & son connor), between the world at home and the world at \"work\", between good and evil, between those who pretend to be men of god and those who really are, between \"clean\" money and \"dirty\", between the town of perdition and perdition as hell. and along the way, savor the visual brilliance of cinematographer conrad l. hall (9 nominations, 2 oscars for best cinematography): rain pouring off fedoras, shots through mirrors (especially on swinging doors), tommy-gun flashes from out of the shadows, absent any sound. not only has 75-year-old hall given us perhaps the best cinematic product of his career, but 77-year-old paul newman offers one of his best performances ever.yes ... i may be getting old. but i've seen a lot ... and this is fresh and invigorating. the road to perdition presents a lasting and loving tribute to the gangster genre, to films of the 40s, to dark comic-book figures lurking in the darkness, to villains and heroes, to american film in general. go see it! bin_features [0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: best, Importance: 0.16524961902292898\n",
      "Word: plot, Importance: 0.11207366745047628\n",
      "Word: story, Importance: 0.09994872332136862\n",
      "Word: just, Importance: 0.06623557823612897\n",
      "Word: movie, Importance: 0.04970755187245878\n",
      "Word: really, Importance: 0.04465201238381678\n",
      "Word: way, Importance: 0.04438078822631167\n",
      "Word: good, Importance: 0.036417061982836785\n",
      "Word: movies, Importance: 0.03512809513724889\n",
      "Word: time, Importance: 0.03373639487672594\n",
      "Word: film, Importance: 0.022675365790872138\n",
      "Word: hanks, Importance: 0.004484368147605483\n",
      "Word: genre, Importance: 0.0028506490004897287\n",
      "Word: off, Importance: 0.002089918049355183\n",
      "Word: encourage, Importance: 0.0015078277059224288\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.46690091912248133\n",
      "Word: like, Importance: 0.4356680000790318\n",
      "Word: don, Importance: 0.3878019912638093\n",
      "Word: plot, Importance: 0.32309441102305275\n",
      "Word: story, Importance: 0.19216256152399191\n",
      "Word: acting, Importance: 0.1865354086778377\n",
      "Word: good, Importance: 0.17644771632451106\n",
      "Word: great, Importance: 0.16327719725613987\n",
      "Word: film, Importance: 0.12937246289044346\n",
      "Word: make, Importance: 0.1278369965181354\n",
      "Word: way, Importance: 0.10440144949480734\n",
      "Word: just, Importance: 0.09628486948221071\n",
      "Word: movies, Importance: 0.08396727558603645\n",
      "Word: time, Importance: 0.07879688481432756\n",
      "Word: watch, Importance: 0.052411017845132024\n",
      "Word: really, Importance: 0.04400940773619577\n",
      "Word: people, Importance: 0.03812940425596478\n",
      "Word: movie, Importance: 0.03644581970735339\n",
      "Word: best, Importance: 0.022572522662615113\n",
      "Word: better, Importance: 0.008556155362644202\n",
      "text sample this is so incredibly bad. poor actors. you can tell they're trying really hard to polish a turd, but we all know you can't. the writing is so obvious and facile, it's sad watching them try to sell it. the humor and pacing are so labored, it's hard to believe any of these good actors signed on for this.that said, it's so awful that we're having a hard time looking away from the screen. we just have to know where this trainwreck goes. but that's only because we caught it on tv. if we had actually paid for this, we'd be disgusted. so it gets 2 stars for being at least amusingly/fascinatingly bad. and the incidental music (as opposed to the trying-too-hard indie soundtrack) is laughably reminiscent of an episode of scooby-doo... but not as good. bin_features [0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: bad, Importance: 0.2845338639175539\n",
      "Word: just, Importance: 0.07709922757408644\n",
      "Word: really, Importance: 0.0457926614761271\n",
      "Word: time, Importance: 0.04139735347628423\n",
      "Word: good, Importance: 0.03605359463525859\n",
      "Word: any, Importance: 0.0023006873714468954\n",
      "Word: that, Importance: 0.0012421653352061306\n",
      "Word: music, Importance: 0.0008317497375856461\n",
      "Word: as, Importance: 0.0007309761315995372\n",
      "Word: if, Importance: 0.0006349671242035941\n",
      "Word: it, Importance: 0.0005128449933109633\n",
      "Word: an, Importance: 0.0004269515500822248\n",
      "Word: for, Importance: 0.000385712082891327\n",
      "Word: trying, Importance: 0.0002759377349737899\n",
      "Word: too, Importance: 4.539557842740941e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.5316169864984661\n",
      "Word: time, Importance: 0.2943824988888661\n",
      "Word: good, Importance: 0.2888560311686575\n",
      "Word: better, Importance: 0.25973087775043785\n",
      "Word: just, Importance: 0.25818488735587103\n",
      "Word: make, Importance: 0.2562339557867847\n",
      "Word: like, Importance: 0.1896933306207514\n",
      "Word: plot, Importance: 0.18968198985881285\n",
      "Word: don, Importance: 0.18457510157663085\n",
      "Word: acting, Importance: 0.1743090106340097\n",
      "Word: bad, Importance: 0.1560438291414174\n",
      "Word: people, Importance: 0.11707974670095\n",
      "Word: best, Importance: 0.10013657545705851\n",
      "Word: way, Importance: 0.07734292394990488\n",
      "Word: story, Importance: 0.06581183217067443\n",
      "Word: watch, Importance: 0.059751351045243206\n",
      "Word: film, Importance: 0.04922046458023538\n",
      "Word: really, Importance: 0.04659490235462588\n",
      "Word: movies, Importance: 0.04062513881757196\n",
      "Word: movie, Importance: 0.01780453903711282\n",
      "text sample i chanced upon this movie because i had a free non-new release from blockbuster and needed to grab something quickly, as the store was getting ready to close for the evening. the plain white cover and title intrigued me. i'm a (relatively speaking) \"old\" lady and my son is a young man of 30. i adore movies that are sheer entertainment, such as the sixth sense, interview with a vampire, harry potter and beetlejuice. my son, on the other hand, is a film graduate and enjoys very specialized foreign films, such as those directed by bergman or hertzog. we generally hate each other's movie choices, however, we both watched and loved the movie nothing! it was unlike any movie we'd ever seen before. we're both cynical/critical personality types and we usually crack on movies while we watch them -- but in this case we just laughed and enjoyed the film from start to finish. it is our opinion that if this movie had been promoted and shown in the main stream theaters in the u.s. it would have done very well indeed. bin_features [0 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: just, Importance: 0.0823465362837729\n",
      "Word: movie, Importance: 0.06097569258597639\n",
      "Word: movies, Importance: 0.04719005262967389\n",
      "Word: film, Importance: 0.03130800317100914\n",
      "Word: watch, Importance: 0.02498143687432953\n",
      "Word: is, Importance: 0.0009092088418694622\n",
      "Word: free, Importance: 0.0007756918300342785\n",
      "Word: u, Importance: 0.0006833182139143119\n",
      "Word: entertainment, Importance: 0.0006465231083904183\n",
      "Word: the, Importance: 0.0006250350449378869\n",
      "Word: relatively, Importance: 0.000509435928025204\n",
      "Word: was, Importance: 0.0003578221117085619\n",
      "Word: usually, Importance: 0.0003080375846767212\n",
      "Word: directed, Importance: 0.0002595546173504744\n",
      "Word: of, Importance: 0.00025711019862534936\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.3397788265549763\n",
      "Word: story, Importance: 0.2574524657407223\n",
      "Word: bad, Importance: 0.2566944046656148\n",
      "Word: best, Importance: 0.24814406669920547\n",
      "Word: don, Importance: 0.19237459127523532\n",
      "Word: really, Importance: 0.1486486785582915\n",
      "Word: like, Importance: 0.13771094156573443\n",
      "Word: film, Importance: 0.12167961890329354\n",
      "Word: better, Importance: 0.10612220004542638\n",
      "Word: watch, Importance: 0.10518912242239226\n",
      "Word: acting, Importance: 0.09200182109402932\n",
      "Word: plot, Importance: 0.08571581433913233\n",
      "Word: way, Importance: 0.0723240431820022\n",
      "Word: just, Importance: 0.0437683960703798\n",
      "Word: time, Importance: 0.034646730709997964\n",
      "Word: good, Importance: 0.03342704397541851\n",
      "Word: movie, Importance: 0.027091295860123155\n",
      "Word: people, Importance: 0.01432563613091875\n",
      "Word: make, Importance: 0.0020093382168430707\n",
      "Word: movies, Importance: 0.0\n",
      "text sample although the plot was a bit sappy at times, and very rushed at the end, as if the director had run out of his alloted time and needed to hurry up and finish the story, overall it was pretty good for the made-for-backwoods-cable-tv genre. however, the actress who played the babysitter, mariana klaveno, was very good! i hope to see more of her around in movie-land. the music was also well done, getting every possible chill out of the dah-duh-dah-duh (think \"jaws\") type music-based tension build-ups.i don't think i'd want to watch \"while the children sleep\" again, but if i did, it would be to focus on the performance of the talented klaveno. bin_features [0 0 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 0]\n",
      "Classical LIME Explanation:\n",
      "Word: don, Importance: 0.15236498973775964\n",
      "Word: plot, Importance: 0.13834886365541474\n",
      "Word: story, Importance: 0.10916032690800045\n",
      "Word: movie, Importance: 0.059150294148447094\n",
      "Word: time, Importance: 0.04270827888859428\n",
      "Word: good, Importance: 0.04069682352950177\n",
      "Word: watch, Importance: 0.02399207566854915\n",
      "Word: hope, Importance: 0.0014996171593635885\n",
      "Word: more, Importance: 0.0014419753190754467\n",
      "Word: up, Importance: 0.0011170695651159972\n",
      "Word: was, Importance: 0.0009295923408099918\n",
      "Word: music, Importance: 0.0008898241040170297\n",
      "Word: again, Importance: 0.0007049134479178204\n",
      "Word: end, Importance: 0.0006104649788018277\n",
      "Word: who, Importance: 0.0005010690205714424\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.4521733668474155\n",
      "Word: good, Importance: 0.2962052749129539\n",
      "Word: film, Importance: 0.28159129880630507\n",
      "Word: like, Importance: 0.2553102549799421\n",
      "Word: bad, Importance: 0.2529939954935774\n",
      "Word: time, Importance: 0.2065204595365705\n",
      "Word: make, Importance: 0.19397901324322667\n",
      "Word: way, Importance: 0.15588475111447975\n",
      "Word: story, Importance: 0.12325345480641103\n",
      "Word: better, Importance: 0.07987479872580588\n",
      "Word: plot, Importance: 0.07144458010545052\n",
      "Word: really, Importance: 0.07034075658734806\n",
      "Word: just, Importance: 0.06626324565275915\n",
      "Word: best, Importance: 0.055620255268273966\n",
      "Word: movies, Importance: 0.04608592170361564\n",
      "Word: people, Importance: 0.04406899108947071\n",
      "Word: movie, Importance: 0.04233163362000736\n",
      "Word: don, Importance: 0.0321580443821784\n",
      "Word: acting, Importance: 0.029049388566191547\n",
      "Word: watch, Importance: 0.023712576684895903\n",
      "text sample i had no idea that mr. izzard was so damn funny, it really boggles the mind that he is not more well known! his command over the crowd and his timing is perfect.the monologue about star wars will kill ya too! if only all the stand up performers had his wit... bin_features [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: really, Importance: 0.04970427882260671\n",
      "Word: mind, Importance: 0.000601387889827284\n",
      "Word: had, Importance: 0.0005186098690561513\n",
      "Word: funny, Importance: 0.0003332743711391375\n",
      "Word: his, Importance: 0.00032243262347733816\n",
      "Word: well, Importance: 0.00031545464661288455\n",
      "Word: perfect, Importance: 0.00029777922940095735\n",
      "Word: damn, Importance: 0.00027982948432142145\n",
      "Word: not, Importance: 0.00027535540105052\n",
      "Word: wars, Importance: 0.00024799092353176567\n",
      "Word: known, Importance: 0.00018169548037453678\n",
      "Word: idea, Importance: 0.00016816372501265927\n",
      "Word: he, Importance: 0.0001528333737459668\n",
      "Word: is, Importance: 0.0001438155064035527\n",
      "Word: all, Importance: 0.00014380421700213063\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.29931726312541823\n",
      "Word: plot, Importance: 0.2022785915144601\n",
      "Word: acting, Importance: 0.1912590463682745\n",
      "Word: great, Importance: 0.188041096266358\n",
      "Word: best, Importance: 0.16487872999321462\n",
      "Word: don, Importance: 0.16176499641502268\n",
      "Word: just, Importance: 0.13833866974744868\n",
      "Word: better, Importance: 0.12581794377619493\n",
      "Word: movie, Importance: 0.11504043781306006\n",
      "Word: story, Importance: 0.10357877184864572\n",
      "Word: people, Importance: 0.1021620351382424\n",
      "Word: make, Importance: 0.0859157113166984\n",
      "Word: really, Importance: 0.052231630802721196\n",
      "Word: film, Importance: 0.031406823547613594\n",
      "Word: watch, Importance: 0.025793805642642775\n",
      "Word: like, Importance: 0.019709811659379017\n",
      "Word: good, Importance: 0.010265646593512368\n",
      "Word: time, Importance: 0.005526467720208594\n",
      "Word: movies, Importance: 0.004210087077620384\n",
      "Word: way, Importance: 0.001115094023101726\n",
      "text sample after all the crap that hollywood (and the indies) have churned out, we finally get a movie that delivers some scary moments. there are some clich√©d moments, but i'm not sure it's possible nowadays to make an entirely original movie. there's not much new here...it's just done well.make sure and pay attention, as the \"subtle\" scares come quickly and often. this is not a movie to watch while you're eating pizza.there's one very well-written red herring in this movie and, unfortunately, one very poorly-cast role. cheri christian just doesn't make an effective julie (the wife/mother). for one thing, she's totally unsympathetic. i know, i know...she's just gone through a traumatic experience. but the viewer never gets to know her as she \"normally\" is and the relationship between her and her husband is rather discomforting (in an unintentional way). i think that the director had meant for us to have some sympathy for her, but i never did.finally, a thumbs-up for the ending, which is both disturbing and satisfying. it could easily have been cheapened with a sound effect at the beginning of the end credits, but the director wisely resisted.this is not a masterpiece by any means, but it is a good, old-fashioned scary movie...something that's rather rare nowadays. bin_features [0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 0 0 1 1]\n",
      "Classical LIME Explanation:\n",
      "Word: make, Importance: 0.0834220876252346\n",
      "Word: just, Importance: 0.08201141747163584\n",
      "Word: movie, Importance: 0.05959445876011376\n",
      "Word: way, Importance: 0.05038085600410844\n",
      "Word: good, Importance: 0.040316525218736694\n",
      "Word: watch, Importance: 0.02514818710366638\n",
      "Word: some, Importance: 0.0010511050761282605\n",
      "Word: and, Importance: 0.0009504872247308331\n",
      "Word: attention, Importance: 0.000689706124752405\n",
      "Word: totally, Importance: 0.00046100099455730265\n",
      "Word: have, Importance: 0.0004401394237723235\n",
      "Word: cheapened, Importance: 0.00036053962594190454\n",
      "Word: with, Importance: 0.00022454975075661103\n",
      "Word: at, Importance: 0.0002140779175386793\n",
      "Word: out, Importance: 0.00017761918222092437\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.3332310781180754\n",
      "Word: best, Importance: 0.28679421488539136\n",
      "Word: bad, Importance: 0.27930841344439594\n",
      "Word: acting, Importance: 0.2507242721185581\n",
      "Word: plot, Importance: 0.20832279731531117\n",
      "Word: way, Importance: 0.20481690849122647\n",
      "Word: story, Importance: 0.19282612039389624\n",
      "Word: don, Importance: 0.1770297514207902\n",
      "Word: movie, Importance: 0.17199817768928338\n",
      "Word: really, Importance: 0.1585280157550223\n",
      "Word: movies, Importance: 0.1274415426008082\n",
      "Word: like, Importance: 0.06963372280623437\n",
      "Word: just, Importance: 0.05303076900010584\n",
      "Word: watch, Importance: 0.05303076900010584\n",
      "Word: people, Importance: 0.03814137168088566\n",
      "Word: good, Importance: 0.035282855016933345\n",
      "Word: film, Importance: 0.02824973275683562\n",
      "Word: time, Importance: 0.013029277216093038\n",
      "Word: make, Importance: 0.0101969013600674\n",
      "Word: better, Importance: 0.006611240083133463\n",
      "text sample when i rented this i was hoping for what \"reign of fire\" did not deliver: a clash between modern technology and mythic beasts.instead i got a standard \"monster hunts stupid people in remote building\" flick, with bad script, bad music, bad effects, bad plot, bad acting. bad, bad, bad.only reason why i did give it a 2 was that in theory there could exist worse movies. in theory..... bin_features [1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: bad, Importance: 0.20565301444821993\n",
      "Word: acting, Importance: 0.12374563733920077\n",
      "Word: plot, Importance: 0.10481600530437629\n",
      "Word: movies, Importance: 0.036195378942578106\n",
      "Word: people, Importance: 0.034932358946835156\n",
      "Word: reason, Importance: 0.008251666108532923\n",
      "Word: mythic, Importance: 0.0075771856827081515\n",
      "Word: only, Importance: 0.00743901148665006\n",
      "Word: modern, Importance: 0.007395026802913294\n",
      "Word: rented, Importance: 0.007382139895517881\n",
      "Word: instead, Importance: 0.006818711094965611\n",
      "Word: exist, Importance: 0.00541817624758543\n",
      "Word: building, Importance: 0.0046157532072268305\n",
      "Word: flick, Importance: 0.004189939508803388\n",
      "Word: not, Importance: 0.0017844300989783208\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: time, Importance: 0.532513045241816\n",
      "Word: great, Importance: 0.4860253606234269\n",
      "Word: watch, Importance: 0.4630071171702536\n",
      "Word: like, Importance: 0.3689617222252146\n",
      "Word: plot, Importance: 0.2936459086675525\n",
      "Word: movie, Importance: 0.27484519896699366\n",
      "Word: better, Importance: 0.26121779888806734\n",
      "Word: film, Importance: 0.2567609643430032\n",
      "Word: movies, Importance: 0.24755946094066889\n",
      "Word: way, Importance: 0.23767151518359708\n",
      "Word: story, Importance: 0.20778209134866926\n",
      "Word: best, Importance: 0.20072147722518496\n",
      "Word: really, Importance: 0.16721550022199927\n",
      "Word: don, Importance: 0.08730655384036225\n",
      "Word: acting, Importance: 0.06308190009560231\n",
      "Word: good, Importance: 0.0597846892312323\n",
      "Word: bad, Importance: 0.05962576035885715\n",
      "Word: people, Importance: 0.05962576035885715\n",
      "Word: make, Importance: 0.036127067115648136\n",
      "Word: just, Importance: 0.004830153680984398\n",
      "text sample warning! spoilers ahead! spoilers i've seen movie in german, so it might be, that i missed some clues. despite some weakness in the plot, it's a movie that came through to me. i liked especially lexa doig's acting. sometimes i got impression, that she *is* camille. but i can't stop wondering, what happened at the end with bob, cassie and baby. i belive, she, after initially being set on bob, eventually ended up loving him and regretting what happened with his brother and being forced to lie to him. otherwise it's a bit strange, that she would carry his baby and love it. it's up to viewer to decide - and i don't like such endings. dean cain was as good as ever, eric roberts .. well, i've seen him better but also worse. i believe that the film is more an analysis of human relations and reacting in unexpected situations than a crime story. bottom line is, i liked it very much. bin_features [1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: acting, Importance: 0.15139003850609556\n",
      "Word: don, Importance: 0.12606589403411905\n",
      "Word: plot, Importance: 0.11178632014185261\n",
      "Word: better, Importance: 0.10126852788924359\n",
      "Word: story, Importance: 0.09332672796348825\n",
      "Word: movie, Importance: 0.053893447298492936\n",
      "Word: good, Importance: 0.0400779066686479\n",
      "Word: film, Importance: 0.02875130623349154\n",
      "Word: like, Importance: 0.02160727217579024\n",
      "Word: missed, Importance: 0.010802164746423\n",
      "Word: despite, Importance: 0.00907362425806788\n",
      "Word: also, Importance: 0.0064127937126969995\n",
      "Word: his, Importance: 0.006236967882580196\n",
      "Word: seen, Importance: 0.005773089362979014\n",
      "Word: roberts, Importance: 0.00036780437163177907\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: great, Importance: 0.5045816403711938\n",
      "Word: just, Importance: 0.42007933682363596\n",
      "Word: watch, Importance: 0.3932166708437125\n",
      "Word: movies, Importance: 0.32617301498765994\n",
      "Word: people, Importance: 0.325174357842158\n",
      "Word: film, Importance: 0.28131647811054994\n",
      "Word: best, Importance: 0.2779293160634001\n",
      "Word: plot, Importance: 0.2119194251443222\n",
      "Word: acting, Importance: 0.17017751473389972\n",
      "Word: make, Importance: 0.16856929722966188\n",
      "Word: like, Importance: 0.1599897993889878\n",
      "Word: better, Importance: 0.13537545206982157\n",
      "Word: really, Importance: 0.13456763923706352\n",
      "Word: bad, Importance: 0.1141976349110663\n",
      "Word: story, Importance: 0.09395771311034021\n",
      "Word: don, Importance: 0.09368775030990256\n",
      "Word: movie, Importance: 0.06705341544945756\n",
      "Word: way, Importance: 0.04388911211710818\n",
      "Word: good, Importance: 0.038755654204752826\n",
      "Word: time, Importance: 0.029418252411843565\n",
      "text sample i can't see the point in burying a movie like this in sulfuric sarcasm, when it is in no way intended to be anything more than a vehicle to entertain children and prepare them for the next line of merchandise to beg madly about. this is a fun movie. my children sat quietly through the entire thing and loved every minute of it. granted, the villain is a bit over the top with his silly costume and maniacal laugher, but this is a lot more easier to take than the dark, gloomy, and very morbid pokemon 3. my children have been watching pokemon since it started and they are soon getting to the ages where they will \"put off the childish things\" and move on to others. i am glad that we got to enjoy this together. bin_features [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1]\n",
      "Classical LIME Explanation:\n",
      "Word: movie, Importance: 0.05827671724065062\n",
      "Word: way, Importance: 0.05021685029860637\n",
      "Word: like, Importance: 0.03158420896443855\n",
      "Word: together, Importance: 0.0004761175549039599\n",
      "Word: gloomy, Importance: 0.000411916132002799\n",
      "Word: madly, Importance: 0.00039484696440393753\n",
      "Word: thing, Importance: 0.00023257414609944736\n",
      "Word: pokemon, Importance: 0.00015724791284751024\n",
      "Word: but, Importance: 0.00011098671534978968\n",
      "Word: when, Importance: 0.0001037467915799601\n",
      "Word: beg, Importance: 0.00010012923779936329\n",
      "Word: off, Importance: 9.54334804747636e-05\n",
      "Word: merchandise, Importance: 7.339407572068856e-05\n",
      "Word: got, Importance: 2.917359628302428e-05\n",
      "Word: for, Importance: 7.421897221514135e-07\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.3161926057650079\n",
      "Word: great, Importance: 0.1988170718732598\n",
      "Word: don, Importance: 0.186116319279773\n",
      "Word: best, Importance: 0.173134998109021\n",
      "Word: story, Importance: 0.15765180523440714\n",
      "Word: better, Importance: 0.15084091812231065\n",
      "Word: acting, Importance: 0.12976727162472607\n",
      "Word: just, Importance: 0.11904613642187262\n",
      "Word: make, Importance: 0.08740942784627126\n",
      "Word: plot, Importance: 0.0865068094644772\n",
      "Word: people, Importance: 0.08286256089477007\n",
      "Word: movie, Importance: 0.05914359635254163\n",
      "Word: time, Importance: 0.045743840950748016\n",
      "Word: good, Importance: 0.04233506232546547\n",
      "Word: like, Importance: 0.03274939247981068\n",
      "Word: movies, Importance: 0.02511149287678227\n",
      "Word: way, Importance: 0.02291005084831854\n",
      "Word: film, Importance: 0.022334462654059584\n",
      "Word: watch, Importance: 0.003527774311759879\n",
      "Word: really, Importance: 0.0011454747114427732\n",
      "text sample there are movies, and there are films. movies are more often than not merely cinematic \"candy,\" whereas films are true works of art. fraulein doktor is certainly well-placed in the latter. as most viewers, i was highly impressed with the battle scenes, but the poignancy of the portrayal of the central character is what i consider to be the most sterling quality of the film. having done everything possible to serve her country as a true daughter of deutschland, all the while in the throes of morphine addiction, die fraulein is treated very shabbily by the german high command despite all of her efforts. the scene in which the doktor is being conveyed in the rear seat of a mercedes benz command auto, alone, desolate, and sobbing is perhaps one of the saddest yet truest depictions of a \"spy's\" lot in life. only the emotional pain presented by richard burton in the spy who came in from the cold comes close. fraulein doktor is a far deeper film than one may realize upon a singular viewing. i only wish that its producers would see fit to release it on dvd so that those who have never experienced it can, and those who have seen it can again (perhaps again and again)enjoy this exceptional motion picture. bin_features [0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Classical LIME Explanation:\n",
      "Word: movies, Importance: 0.046809004537239074\n",
      "Word: film, Importance: 0.03073690904587968\n",
      "Word: rear, Importance: 0.00019814940612789957\n",
      "Word: perhaps, Importance: 0.00016796877349604768\n",
      "Word: picture, Importance: 0.00015795874456181908\n",
      "Word: german, Importance: 0.00014540020464609313\n",
      "Word: fraulein, Importance: 0.00014092466254245938\n",
      "Word: certainly, Importance: 0.00010914286711508453\n",
      "Word: comes, Importance: 0.00010055219854419\n",
      "Word: character, Importance: 8.309411924977561e-05\n",
      "Word: close, Importance: 7.496629798415383e-05\n",
      "Word: depictions, Importance: 7.26561504320311e-05\n",
      "Word: desolate, Importance: 6.701142243599593e-05\n",
      "Word: which, Importance: 3.941624570729494e-05\n",
      "Word: release, Importance: 1.0029620715991525e-05\n",
      "\n",
      "Q-LIME Pi Explanation:\n",
      "Word: bad, Importance: 0.26752758336077465\n",
      "Word: great, Importance: 0.2519997991765971\n",
      "Word: don, Importance: 0.21134158972129657\n",
      "Word: better, Importance: 0.17686201813601782\n",
      "Word: best, Importance: 0.16246572103282375\n",
      "Word: acting, Importance: 0.1599334463569424\n",
      "Word: plot, Importance: 0.11703247369263825\n",
      "Word: story, Importance: 0.10877989905705443\n",
      "Word: make, Importance: 0.10461341072453101\n",
      "Word: just, Importance: 0.10259753542562877\n",
      "Word: really, Importance: 0.081211009968716\n",
      "Word: way, Importance: 0.08016094199074353\n",
      "Word: movie, Importance: 0.07929930349124015\n",
      "Word: good, Importance: 0.07152675344265536\n",
      "Word: people, Importance: 0.04953340574193221\n",
      "Word: like, Importance: 0.0320840743295171\n",
      "Word: movies, Importance: 0.03153104724419953\n",
      "Word: film, Importance: 0.016490496480901284\n",
      "Word: watch, Importance: 0.009947328679177136\n",
      "Word: time, Importance: 0.0013542162874563557\n",
      "Results => {'num_samples': 500, 'max_features': 20, 'stopwords': True, 'lime_num_samples': 300, 'shots': 100, 'local_accuracy': np.float64(0.6), 'lime_time_avg': np.float64(0.1442), 'qlime_time_avg': np.float64(5.4369), 'overlap_avg': np.float64(0.6), 'n_test_explanations': 5, 'stop_words': 'english'}\n",
      "\n",
      "All done! Saved results to 'results_expanded_flips.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "\n",
    "    # Parameter grid to systematically vary certain settings\n",
    "    param_grid = {\n",
    "        \"num_samples\": [500],\n",
    "        \"max_features\": [20],\n",
    "        \"stopwords_option\": [True],\n",
    "        \"lime_num_samples\": [300],\n",
    "        # Shots: None => analytic mode, 100 => finite sampling\n",
    "        \"shots\": [100],\n",
    "        \"stop_words\": ['english'],\n",
    "        \"n_test_explanations\": [5]\n",
    "    }\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    all_results = []\n",
    "\n",
    "    for combo in combos:\n",
    "        (num_samples_, max_features_, stopwords_, lime_samps_, shots_, stop_words_, n_test_explanations_) = combo\n",
    "        \n",
    "        print(\"\\n==================================\")\n",
    "        print(f\"Running experiment with: \"\n",
    "              f\"num_samples={num_samples_}, \"\n",
    "              f\"max_features={max_features_}, \"\n",
    "              f\"stopwords={stopwords_}, \"\n",
    "              f\"lime_num_samples={lime_samps_}, \"\n",
    "              f\"shots={shots_},\"\n",
    "              f\"stop_words={stop_words_},\"\n",
    "              f\"n_test_explanations={n_test_explanations_}\")\n",
    "        \n",
    "        res = run_experiment(\n",
    "            num_samples=num_samples_,\n",
    "            max_features=max_features_,\n",
    "            stopwords_option=stopwords_,\n",
    "            lime_num_samples=lime_samps_,\n",
    "            shots=shots_,\n",
    "            stop_words=stop_words_,\n",
    "            n_test_explanations=n_test_explanations_,\n",
    "            \n",
    "            \n",
    "        )\n",
    "        res_row = {\n",
    "            \"num_samples\": num_samples_,\n",
    "            \"max_features\": max_features_,\n",
    "            \"stopwords\": stopwords_,\n",
    "            \"lime_num_samples\": lime_samps_,\n",
    "            \"shots\": shots_,\n",
    "            \"local_accuracy\": res[\"local_accuracy\"],\n",
    "            \"lime_time_avg\": res[\"lime_time_avg\"],\n",
    "            \"qlime_time_avg\": res[\"qlime_time_avg\"],\n",
    "            \"overlap_avg\": res[\"overlap_avg\"],\n",
    "            \"n_test_explanations\": n_test_explanations_,\n",
    "            \"stop_words\": stop_words_\n",
    "        }\n",
    "        print(\"Results =>\", res_row)\n",
    "        all_results.append(res_row)\n",
    "\n",
    "    # Save results to CSV\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df.to_csv(\"results_expanded_flips.csv\", index=False)\n",
    "    print(\"\\nAll done! Saved results to 'results_expanded_flips.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bell_inequality)",
   "language": "python",
   "name": "bell_inequality"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
